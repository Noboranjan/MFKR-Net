{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 21:50:57.441406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 21:50:58.158256: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bigdata_user5/Desktop/envs/nobo/lib/\n",
      "2023-05-20 21:50:58.158326: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bigdata_user5/Desktop/envs/nobo/lib/\n",
      "2023-05-20 21:50:58.158332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-d456bb71-756c-5f6b-b13b-6793b90de8b7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7980\n",
      "7980\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down')\n",
    "\n",
    "train_subject_types = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down')\n",
    "\n",
    "input_images=[]\n",
    "input_labels=[]\n",
    "\n",
    "train_type = ['middlefinger']\n",
    "train_set=['set1','set2','set3','set4','set5','set6']\n",
    "\n",
    "def scan_folder(train_dataset_path,label):\n",
    "    images = []\n",
    "    labels=[]\n",
    "    complete_path=[x[0] for x in os.walk(train_dataset_path)]\n",
    "    for file_name in complete_path:\n",
    "        #print(file_name)\n",
    "        for fn in  os.listdir(file_name) :\n",
    "            # print(fn)\n",
    "            if fn.endswith(\".bmp\"):\n",
    "            # if it's a txt file, print its name (or do whatever you want)\n",
    "                # print(file_name)\n",
    "                #im = Image.open(file_name+'/'+fn)\n",
    "                im = cv2.imread(file_name+'/'+fn)\n",
    "                # im.show()\n",
    "                #img_arr=cv2.resize(im,(160,160))\n",
    "                img_arr=im \n",
    "                images.append(img_arr)\n",
    "                labels.append(label)\n",
    "                #print(im.size)\n",
    "                #print(im.filename)\n",
    "             \n",
    "        \n",
    "        else:\n",
    "            current_path = \"\".join((train_dataset_path, \"/\", file_name))\n",
    "            if os.path.isdir(current_path):\n",
    "                # if we're checking a sub-directory, recursively call this method\n",
    "                scan_folder(current_path)\n",
    "    #print(images)\n",
    "    return images,labels\n",
    "\n",
    "\n",
    "for item1 in train_subject_types:\n",
    "    all_train_subjects = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down' + '/' +item1)\n",
    "    #print(all_subjects)\n",
    "    for item2 in train_type:\n",
    "        for item3 in train_set:\n",
    "            path=str('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down'  + '/' +item1 +  '/session1'+ '/' +item2 + '/'+item3+ '/3D')\n",
    "            img,label=scan_folder(path,item1)\n",
    "            input_images.extend(img)\n",
    "            input_labels.extend(label)\n",
    "            #print(input_labels)\n",
    "\n",
    "input_train_images=np.array(input_images)  \n",
    "input_train_labels=np.array(input_labels)\n",
    "print(len(input_train_images))\n",
    "print(len(input_train_labels))\n",
    "#print(input_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7980, 149, 212, 3)\n",
      "(7980,)\n",
      "7980\n",
      "7980\n"
     ]
    }
   ],
   "source": [
    "test_dataset_path = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down')\n",
    "\n",
    "test_subject_types = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down')\n",
    "\n",
    "input_images=[]\n",
    "input_labels=[]\n",
    "\n",
    "test_type = ['middlefinger']\n",
    "test_set=['set1','set2','set3','set4','set5','set6']\n",
    "\n",
    "\n",
    "def scan_folder(test_dataset_path,label):\n",
    "    images = []\n",
    "    labels=[]\n",
    "    #print(test_dataset_path)\n",
    "    complete_path=[x[0] for x in os.walk(test_dataset_path)]\n",
    "    #print(complete_path)\n",
    "    for file_name in complete_path:\n",
    "        #print(file_name)\n",
    "        for fn in  os.listdir(file_name) :\n",
    "            # print(fn)\n",
    "            if fn.endswith(\".bmp\"):\n",
    "            # if it's a txt file, print its name (or do whatever you want)\n",
    "                # print(file_name)\n",
    "                #im = Image.open(file_name+'/'+fn)\n",
    "                im = cv2.imread(file_name+'/'+fn)\n",
    "                # im.show()\n",
    "                img_arr=cv2.resize(im,(212,149))\n",
    "                #img_arr=im\n",
    "                images.append(img_arr)\n",
    "                labels.append(label)\n",
    "             \n",
    "        \n",
    "        else:\n",
    "            current_path = \"\".join((test_dataset_path, \"/\", file_name))\n",
    "            if os.path.isdir(current_path):\n",
    "                # if we're checking a sub-directory, recursively call this method\n",
    "                scan_folder(current_path)\n",
    "    #print(images)\n",
    "    return images,labels\n",
    "\n",
    "\n",
    "for item1 in train_subject_types:\n",
    "    #all_train_subjects = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down' + '/' +item1)\n",
    "    #print(all_train_subjects)\n",
    "    for item2 in test_type:\n",
    "        for item3 in test_set:\n",
    "            path=str('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down'  + '/' +item1 +  '/session2'+ '/' +item2 + '/'+item3+ '/3D')\n",
    "            img,label=scan_folder(path,item1)\n",
    "            input_images.extend(img)\n",
    "            input_labels.extend(label)\n",
    "            # print(input_labels)\n",
    "\n",
    "input_test_images=np.array(input_images) #,axis=0)\n",
    "input_test_labels=np.array(input_labels)\n",
    "# print(input_labels.shape)\n",
    "# input_test_labels=np.concatenate(np.array(input_labels),axis=0)\n",
    "print(input_test_images.shape)\n",
    "print(input_test_labels.shape)\n",
    "print(len(input_test_images))    \n",
    "print(len(input_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7980, 149, 212, 3)\n",
      "(7980,)\n",
      "(7980, 149, 212, 3)\n",
      "(7980,)\n"
     ]
    }
   ],
   "source": [
    "X_train = input_train_images\n",
    "Y_train = input_train_labels\n",
    "\n",
    "X_test = input_test_images\n",
    "Y_test = input_test_labels\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train/22.0\n",
    "# X_test = X_test/35.0\n",
    "\n",
    "# X_train[1,:]\n",
    "# X_test[1,:]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb= LabelEncoder()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "Y_test = lb.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "WARNING:tensorflow:From /tmp/ipykernel_3970716/3934639393.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 21:51:26.563377: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 21:51:27.025598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 38541 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB MIG 3g.40gb, pci bus id: 0000:c2:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 21:51:29.542152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38541 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB MIG 3g.40gb, pci bus id: 0000:c2:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "img_height,img_width = 149,212 \n",
    "#If imagenet weights are being loaded, \n",
    "#input must have a static square shape (one of (128, 128), (160, 160), (192, 192), or (224, 224))\n",
    "base_model = tf.keras.applications.Xception(weights=\"imagenet\", include_top=False, input_shape= (img_height,img_width,3))\n",
    "\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "predictions = Dense(190, activation= 'softmax')(x)\n",
    "model = tf.keras.Model(inputs = base_model.input, outputs = predictions)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 21:51:42.215051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-05-20 21:51:45.015554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-20 21:51:45.017809: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x4aa49a90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-20 21:51:45.017844: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB MIG 3g.40gb, Compute Capability 8.0\n",
      "2023-05-20 21:51:45.022598: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-20 21:51:45.136077: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 47s 342ms/step - loss: 1.8560 - accuracy: 0.6716\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 20s 313ms/step - loss: 0.0095 - accuracy: 0.9996\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 20s 313ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 20s 316ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 20s 313ms/step - loss: 7.8330e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 6s 81ms/step - loss: 1.8549 - accuracy: 0.5890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8548985719680786, 0.5889724493026733]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/Xception.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('model/Xception_training_history.npy', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 8s 25ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.3571    0.5263        42\n",
      "           1     0.8333    0.1190    0.2083        42\n",
      "           2     0.3077    0.7619    0.4384        42\n",
      "           3     0.7593    0.9762    0.8542        42\n",
      "           4     0.4330    1.0000    0.6043        42\n",
      "           5     0.4118    1.0000    0.5833        42\n",
      "           6     1.0000    1.0000    1.0000        42\n",
      "           7     0.1930    0.2619    0.2222        42\n",
      "           8     0.6000    1.0000    0.7500        42\n",
      "           9     0.4828    1.0000    0.6512        42\n",
      "          10     0.4516    1.0000    0.6222        42\n",
      "          11     0.6429    0.4286    0.5143        42\n",
      "          12     0.0000    0.0000    0.0000        42\n",
      "          13     1.0000    0.9762    0.9880        42\n",
      "          14     0.9767    1.0000    0.9882        42\n",
      "          15     0.3889    1.0000    0.5600        42\n",
      "          16     0.6000    1.0000    0.7500        42\n",
      "          17     0.5833    1.0000    0.7368        42\n",
      "          18     0.9722    0.8333    0.8974        42\n",
      "          19     0.8936    1.0000    0.9438        42\n",
      "          20     0.7119    1.0000    0.8317        42\n",
      "          21     0.5538    0.8571    0.6729        42\n",
      "          22     0.8750    1.0000    0.9333        42\n",
      "          23     0.6667    0.0476    0.0889        42\n",
      "          24     0.3883    0.9524    0.5517        42\n",
      "          25     0.8936    1.0000    0.9438        42\n",
      "          26     0.8400    1.0000    0.9130        42\n",
      "          27     0.6364    1.0000    0.7778        42\n",
      "          28     1.0000    1.0000    1.0000        42\n",
      "          29     0.5574    0.8095    0.6602        42\n",
      "          30     0.3267    0.7857    0.4615        42\n",
      "          31     0.9130    1.0000    0.9545        42\n",
      "          32     0.6562    1.0000    0.7925        42\n",
      "          33     0.9767    1.0000    0.9882        42\n",
      "          34     1.0000    0.6429    0.7826        42\n",
      "          35     0.4302    0.8810    0.5781        42\n",
      "          36     0.4330    1.0000    0.6043        42\n",
      "          37     0.2710    1.0000    0.4264        42\n",
      "          38     0.6885    1.0000    0.8155        42\n",
      "          39     0.8864    0.9286    0.9070        42\n",
      "          40     0.7500    1.0000    0.8571        42\n",
      "          41     0.8261    0.9048    0.8636        42\n",
      "          42     0.5185    1.0000    0.6829        42\n",
      "          43     0.3115    0.9048    0.4634        42\n",
      "          44     0.1900    1.0000    0.3194        42\n",
      "          45     1.0000    0.1667    0.2857        42\n",
      "          46     0.7069    0.9762    0.8200        42\n",
      "          47     0.0000    0.0000    0.0000        42\n",
      "          48     0.7241    1.0000    0.8400        42\n",
      "          49     0.8913    0.9762    0.9318        42\n",
      "          50     0.7500    1.0000    0.8571        42\n",
      "          51     0.5909    0.9286    0.7222        42\n",
      "          52     0.5676    1.0000    0.7241        42\n",
      "          53     0.7368    1.0000    0.8485        42\n",
      "          54     0.4713    0.9762    0.6357        42\n",
      "          55     0.8571    1.0000    0.9231        42\n",
      "          56     0.9286    0.3095    0.4643        42\n",
      "          57     0.9737    0.8810    0.9250        42\n",
      "          58     0.0811    0.0714    0.0759        42\n",
      "          59     0.2593    0.1667    0.2029        42\n",
      "          60     0.1636    0.2143    0.1856        42\n",
      "          61     0.6735    0.7857    0.7253        42\n",
      "          62     0.2105    0.0952    0.1311        42\n",
      "          63     0.9697    0.7619    0.8533        42\n",
      "          64     0.0893    0.1190    0.1020        42\n",
      "          65     0.0227    0.0238    0.0233        42\n",
      "          66     0.4918    0.7143    0.5825        42\n",
      "          67     1.0000    0.0952    0.1739        42\n",
      "          68     0.4667    1.0000    0.6364        42\n",
      "          69     0.0984    0.1429    0.1165        42\n",
      "          70     0.9767    1.0000    0.9882        42\n",
      "          71     1.0000    1.0000    1.0000        42\n",
      "          72     0.6176    1.0000    0.7636        42\n",
      "          73     0.6176    1.0000    0.7636        42\n",
      "          74     1.0000    0.9762    0.9880        42\n",
      "          75     0.9767    1.0000    0.9882        42\n",
      "          76     0.9688    0.7381    0.8378        42\n",
      "          77     0.6462    1.0000    0.7850        42\n",
      "          78     0.0000    0.0000    0.0000        42\n",
      "          79     0.9767    1.0000    0.9882        42\n",
      "          80     0.8077    1.0000    0.8936        42\n",
      "          81     0.4468    1.0000    0.6176        42\n",
      "          82     0.9545    1.0000    0.9767        42\n",
      "          83     0.3621    1.0000    0.5316        42\n",
      "          84     0.3846    0.4762    0.4255        42\n",
      "          85     0.8750    1.0000    0.9333        42\n",
      "          86     0.7368    1.0000    0.8485        42\n",
      "          87     0.5753    1.0000    0.7304        42\n",
      "          88     0.5250    1.0000    0.6885        42\n",
      "          89     0.0909    0.0238    0.0377        42\n",
      "          90     0.5942    0.9762    0.7387        42\n",
      "          91     0.9767    1.0000    0.9882        42\n",
      "          92     0.7500    1.0000    0.8571        42\n",
      "          93     0.3853    1.0000    0.5563        42\n",
      "          94     0.7500    1.0000    0.8571        42\n",
      "          95     0.2500    0.6190    0.3562        42\n",
      "          96     0.6087    1.0000    0.7568        42\n",
      "          97     0.8936    1.0000    0.9438        42\n",
      "          98     0.5686    0.6905    0.6237        42\n",
      "          99     0.5455    1.0000    0.7059        42\n",
      "         100     1.0000    0.2381    0.3846        42\n",
      "         101     0.7241    1.0000    0.8400        42\n",
      "         102     1.0000    0.3571    0.5263        42\n",
      "         103     0.0000    0.0000    0.0000        42\n",
      "         104     1.0000    0.9524    0.9756        42\n",
      "         105     0.0000    0.0000    0.0000        42\n",
      "         106     0.9091    0.2381    0.3774        42\n",
      "         107     0.0000    0.0000    0.0000        42\n",
      "         108     0.8125    0.6190    0.7027        42\n",
      "         109     0.7073    0.6905    0.6988        42\n",
      "         110     0.0000    0.0000    0.0000        42\n",
      "         111     1.0000    0.1429    0.2500        42\n",
      "         112     0.9333    1.0000    0.9655        42\n",
      "         113     1.0000    0.3333    0.5000        42\n",
      "         114     1.0000    0.0238    0.0465        42\n",
      "         115     1.0000    0.0476    0.0909        42\n",
      "         116     0.0000    0.0000    0.0000        42\n",
      "         117     1.0000    0.4286    0.6000        42\n",
      "         118     0.9032    0.6667    0.7671        42\n",
      "         119     0.0000    0.0000    0.0000        42\n",
      "         120     0.7500    0.1429    0.2400        42\n",
      "         121     0.6667    0.0952    0.1667        42\n",
      "         122     0.2121    0.1667    0.1867        42\n",
      "         123     0.0000    0.0000    0.0000        42\n",
      "         124     0.0000    0.0000    0.0000        42\n",
      "         125     0.0000    0.0000    0.0000        42\n",
      "         126     0.0000    0.0000    0.0000        42\n",
      "         127     0.0000    0.0000    0.0000        42\n",
      "         128     0.5000    0.0714    0.1250        42\n",
      "         129     0.0000    0.0000    0.0000        42\n",
      "         130     1.0000    0.8571    0.9231        42\n",
      "         131     0.0000    0.0000    0.0000        42\n",
      "         132     0.8000    0.6667    0.7273        42\n",
      "         133     0.0000    0.0000    0.0000        42\n",
      "         134     0.9643    0.6429    0.7714        42\n",
      "         135     0.0000    0.0000    0.0000        42\n",
      "         136     1.0000    0.3571    0.5263        42\n",
      "         137     0.0000    0.0000    0.0000        42\n",
      "         138     0.7500    0.4286    0.5455        42\n",
      "         139     1.0000    0.2619    0.4151        42\n",
      "         140     0.8864    0.9286    0.9070        42\n",
      "         141     0.0000    0.0000    0.0000        42\n",
      "         142     0.5000    0.0238    0.0455        42\n",
      "         143     1.0000    0.3571    0.5263        42\n",
      "         144     0.8333    0.1190    0.2083        42\n",
      "         145     0.9091    0.4762    0.6250        42\n",
      "         146     0.8571    0.1429    0.2449        42\n",
      "         147     1.0000    1.0000    1.0000        42\n",
      "         148     1.0000    0.4762    0.6452        42\n",
      "         149     0.9048    0.4524    0.6032        42\n",
      "         150     0.8929    0.5952    0.7143        42\n",
      "         151     1.0000    0.3095    0.4727        42\n",
      "         152     0.0000    0.0000    0.0000        42\n",
      "         153     0.0000    0.0000    0.0000        42\n",
      "         154     0.0000    0.0000    0.0000        42\n",
      "         155     0.9545    0.5000    0.6562        42\n",
      "         156     0.8857    0.7381    0.8052        42\n",
      "         157     0.5000    0.0238    0.0455        42\n",
      "         158     0.6944    0.5952    0.6410        42\n",
      "         159     1.0000    0.7619    0.8649        42\n",
      "         160     0.8333    0.3571    0.5000        42\n",
      "         161     0.1429    0.0238    0.0408        42\n",
      "         162     0.9130    1.0000    0.9545        42\n",
      "         163     0.4304    0.8095    0.5620        42\n",
      "         164     0.0000    0.0000    0.0000        42\n",
      "         165     1.0000    0.4762    0.6452        42\n",
      "         166     0.0000    0.0000    0.0000        42\n",
      "         167     1.0000    0.4286    0.6000        42\n",
      "         168     0.0000    0.0000    0.0000        42\n",
      "         169     0.7143    0.2381    0.3571        42\n",
      "         170     0.3500    0.3333    0.3415        42\n",
      "         171     1.0000    0.6429    0.7826        42\n",
      "         172     1.0000    0.0238    0.0465        42\n",
      "         173     1.0000    0.9762    0.9880        42\n",
      "         174     0.9500    0.4524    0.6129        42\n",
      "         175     0.7018    0.9524    0.8081        42\n",
      "         176     0.9231    0.8571    0.8889        42\n",
      "         177     0.9286    0.9286    0.9286        42\n",
      "         178     0.8163    0.9524    0.8791        42\n",
      "         179     0.0000    0.0000    0.0000        42\n",
      "         180     0.0000    0.0000    0.0000        42\n",
      "         181     1.0000    0.3810    0.5517        42\n",
      "         182     1.0000    0.8810    0.9367        42\n",
      "         183     0.6667    0.0476    0.0889        42\n",
      "         184     0.7857    0.2619    0.3929        42\n",
      "         185     1.0000    0.6190    0.7647        42\n",
      "         186     0.0000    0.0000    0.0000        42\n",
      "         187     0.2471    1.0000    0.3962        42\n",
      "         188     0.4773    1.0000    0.6462        42\n",
      "         189     0.7500    1.0000    0.8571        42\n",
      "\n",
      "    accuracy                         0.5890      7980\n",
      "   macro avg     0.6079    0.5890    0.5347      7980\n",
      "weighted avg     0.6079    0.5890    0.5347      7980\n",
      "\n",
      "[[15  0  0 ...  0  0  0]\n",
      " [ 0  5 20 ...  1  0  0]\n",
      " [ 0  0 32 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 42  0  0]\n",
      " [ 0  0  0 ...  0 42  0]\n",
      " [ 0  0  0 ...  0  0 42]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata_user5/Desktop/envs/nobo/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bigdata_user5/Desktop/envs/nobo/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bigdata_user5/Desktop/envs/nobo/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "cm= confusion_matrix(Y_test,y_pred)\n",
    "# np.set_printoptions(precision=2)\n",
    "cr = classification_report(Y_test, y_pred, digits=4)\n",
    "print(cr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity (TPR) of the model: 0.5889724310776943\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# assume y_true is the true labels and y_pred is the predicted labels\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# calculate total true positives and false negatives\n",
    "tp = np.diag(cm)\n",
    "fn = np.sum(cm, axis=1) - tp\n",
    "\n",
    "# calculate total sensitivity (TPR)\n",
    "sensitivity = np.sum(tp) / (np.sum(tp) + np.sum(fn))\n",
    "\n",
    "print(\"Sensitivity (TPR) of the model:\", sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossover Error Rate (CER): 0.20660115898211137\n",
      "false positive rate for the model: 0.0021747490419169618\n",
      "false negative rate for the model: 0.41102756892230574\n",
      "0.41102756892230574\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# assume y_true is the true labels and y_pred is the predicted labels\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# calculate false positives (FP), false negatives (FN), true positives (TP), and true negatives (TN)\n",
    "FP = np.sum(cm, axis=0) - np.diag(cm)\n",
    "FN = np.sum(cm, axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = np.sum(cm) - (FP + FN + TP)\n",
    "\n",
    "# calculate the False Positive Rate (FPR) and False Negative Rate (FNR)\n",
    "FPR = FP / (FP + TN)\n",
    "FNR = FN / (FN + TP)\n",
    "\n",
    "# calculate the Crossover Error Rate (CER)\n",
    "CER = (FPR + FNR) / 2\n",
    "\n",
    "print(\"Crossover Error Rate (CER):\", np.mean(CER))\n",
    "\n",
    "FPR = np.sum(FP) / np.sum(FP + TN)\n",
    "FNR = np.sum(FN) / np.sum(FN + TP)\n",
    "FDR = np.sum(FP) / np.sum(FP + TP)\n",
    "\n",
    "print(\"false positive rate for the model:\", FPR)\n",
    "print(\"false negative rate for the model:\", FNR)\n",
    "print(FDR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9acf8662ace7bc7f0b52e0ec6110aadb9cc5cd0b0b1cfb4c02b73fd26dabccde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
