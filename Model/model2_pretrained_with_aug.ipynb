{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 20:27:21.146109: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 20:27:21.826402: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bigdata_user5/Desktop/envs/nobo/lib/\n",
      "2023-05-20 20:27:21.826476: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bigdata_user5/Desktop/envs/nobo/lib/\n",
      "2023-05-20 20:27:21.826482: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-d456bb71-756c-5f6b-b13b-6793b90de8b7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7980\n",
      "7980\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down')\n",
    "\n",
    "train_subject_types = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down')\n",
    "\n",
    "input_images=[]\n",
    "input_labels=[]\n",
    "\n",
    "train_type = ['middlefinger']\n",
    "train_set=['set1','set2','set3','set4','set5','set6']\n",
    "\n",
    "def scan_folder(train_dataset_path,label):\n",
    "    images = []\n",
    "    labels=[]\n",
    "    complete_path=[x[0] for x in os.walk(train_dataset_path)]\n",
    "    for file_name in complete_path:\n",
    "        #print(file_name)\n",
    "        for fn in  os.listdir(file_name) :\n",
    "            # print(fn)\n",
    "            if fn.endswith(\".bmp\"):\n",
    "            # if it's a txt file, print its name (or do whatever you want)\n",
    "                # print(file_name)\n",
    "                #im = Image.open(file_name+'/'+fn)\n",
    "                im = cv2.imread(file_name+'/'+fn)\n",
    "                # im.show()\n",
    "                #img_arr=cv2.resize(im,(160,160))\n",
    "                img_arr=im \n",
    "                images.append(img_arr)\n",
    "                labels.append(label)\n",
    "                #print(im.size)\n",
    "                #print(im.filename)\n",
    "             \n",
    "        \n",
    "        else:\n",
    "            current_path = \"\".join((train_dataset_path, \"/\", file_name))\n",
    "            if os.path.isdir(current_path):\n",
    "                # if we're checking a sub-directory, recursively call this method\n",
    "                scan_folder(current_path)\n",
    "    #print(images)\n",
    "    return images,labels\n",
    "\n",
    "\n",
    "for item1 in train_subject_types:\n",
    "    all_train_subjects = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down' + '/' +item1)\n",
    "    #print(all_subjects)\n",
    "    for item2 in train_type:\n",
    "        for item3 in train_set:\n",
    "            path=str('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down'  + '/' +item1 +  '/session1'+ '/' +item2 + '/'+item3+ '/3D')\n",
    "            img,label=scan_folder(path,item1)\n",
    "            input_images.extend(img)\n",
    "            input_labels.extend(label)\n",
    "            #print(input_labels)\n",
    "\n",
    "input_train_images=np.array(input_images)  \n",
    "input_train_labels=np.array(input_labels)\n",
    "print(len(input_train_images))\n",
    "print(len(input_train_labels))\n",
    "#print(input_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7980, 149, 212, 3)\n",
      "(7980,)\n",
      "7980\n",
      "7980\n"
     ]
    }
   ],
   "source": [
    "test_dataset_path = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down')\n",
    "\n",
    "test_subject_types = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down')\n",
    "\n",
    "input_images=[]\n",
    "input_labels=[]\n",
    "\n",
    "test_type = ['middlefinger']\n",
    "test_set=['set1','set2','set3','set4','set5','set6']\n",
    "\n",
    "\n",
    "def scan_folder(test_dataset_path,label):\n",
    "    images = []\n",
    "    labels=[]\n",
    "    #print(test_dataset_path)\n",
    "    complete_path=[x[0] for x in os.walk(test_dataset_path)]\n",
    "    #print(complete_path)\n",
    "    for file_name in complete_path:\n",
    "        #print(file_name)\n",
    "        for fn in  os.listdir(file_name) :\n",
    "            # print(fn)\n",
    "            if fn.endswith(\".bmp\"):\n",
    "            # if it's a txt file, print its name (or do whatever you want)\n",
    "                # print(file_name)\n",
    "                #im = Image.open(file_name+'/'+fn)\n",
    "                im = cv2.imread(file_name+'/'+fn)\n",
    "                # im.show()\n",
    "                img_arr=cv2.resize(im,(212,149))\n",
    "                #img_arr=im\n",
    "                images.append(img_arr)\n",
    "                labels.append(label)\n",
    "             \n",
    "        \n",
    "        else:\n",
    "            current_path = \"\".join((test_dataset_path, \"/\", file_name))\n",
    "            if os.path.isdir(current_path):\n",
    "                # if we're checking a sub-directory, recursively call this method\n",
    "                scan_folder(current_path)\n",
    "    #print(images)\n",
    "    return images,labels\n",
    "\n",
    "\n",
    "for item1 in train_subject_types:\n",
    "    #all_train_subjects = os.listdir('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down' + '/' +item1)\n",
    "    #print(all_train_subjects)\n",
    "    for item2 in test_type:\n",
    "        for item3 in test_set:\n",
    "            path=str('data/3Dfingerknuckle/3D_Finger_Knuckle_Database/segmented_data_down'  + '/' +item1 +  '/session2'+ '/' +item2 + '/'+item3+ '/3D')\n",
    "            img,label=scan_folder(path,item1)\n",
    "            input_images.extend(img)\n",
    "            input_labels.extend(label)\n",
    "            # print(input_labels)\n",
    "\n",
    "input_test_images=np.array(input_images) #,axis=0)\n",
    "input_test_labels=np.array(input_labels)\n",
    "# print(input_labels.shape)\n",
    "# input_test_labels=np.concatenate(np.array(input_labels),axis=0)\n",
    "print(input_test_images.shape)\n",
    "print(input_test_labels.shape)\n",
    "print(len(input_test_images))    \n",
    "print(len(input_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7980, 149, 212, 3)\n",
      "(7980,)\n",
      "(7980, 149, 212, 3)\n",
      "(7980,)\n"
     ]
    }
   ],
   "source": [
    "X_train = input_train_images\n",
    "Y_train = input_train_labels\n",
    "\n",
    "X_test = input_test_images\n",
    "Y_test = input_test_labels\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train/22.0\n",
    "# X_test = X_test/35.0\n",
    "\n",
    "# X_train[1,:]\n",
    "# X_test[1,:]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb= LabelEncoder()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "Y_test = lb.fit_transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "WARNING:tensorflow:From /tmp/ipykernel_3775443/3934639393.py:3: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 20:27:34.548737: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-20 20:27:35.022247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 38541 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB MIG 3g.40gb, pci bus id: 0000:c2:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 20:27:36.953025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38541 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB MIG 3g.40gb, pci bus id: 0000:c2:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "img_height,img_width = 149,212 \n",
    "#If imagenet weights are being loaded, \n",
    "#input must have a static square shape (one of (128, 128), (160, 160), (192, 192), or (224, 224))\n",
    "base_model = tf.keras.applications.resnet50.ResNet50(weights=\"imagenet\", include_top=False, input_shape= (img_height,img_width,3))\n",
    "\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "predictions = Dense(190, activation= 'softmax')(x)\n",
    "model = tf.keras.Model(inputs = base_model.input, outputs = predictions)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 20:27:51.615771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-05-20 20:27:53.824032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-20 20:27:53.825960: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f25fb78d370 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-20 20:27:53.825977: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-80GB MIG 3g.40gb, Compute Capability 8.0\n",
      "2023-05-20 20:27:53.830673: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-20 20:27:53.946185: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 47s 256ms/step - loss: 1.8033 - accuracy: 0.6276\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0669 - accuracy: 0.9850\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - 15s 241ms/step - loss: 0.0202 - accuracy: 0.9954\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0213 - accuracy: 0.9950\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.0723 - accuracy: 0.9816\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.0392 - accuracy: 0.9878\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - 15s 239ms/step - loss: 0.0319 - accuracy: 0.9936\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.0163 - accuracy: 0.9959\n",
      "Epoch 9/25\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 10/25\n",
      "63/63 [==============================] - 15s 234ms/step - loss: 8.3734e-04 - accuracy: 0.9999\n",
      "Epoch 11/25\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 2.8879e-04 - accuracy: 1.0000\n",
      "Epoch 12/25\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 1.5477e-04 - accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "63/63 [==============================] - 15s 234ms/step - loss: 9.8932e-05 - accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "63/63 [==============================] - 15s 234ms/step - loss: 6.6363e-05 - accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 6.8019e-05 - accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 5.2406e-05 - accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 4.6844e-05 - accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 4.1173e-05 - accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 3.4382e-05 - accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 3.0074e-05 - accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "63/63 [==============================] - 15s 234ms/step - loss: 3.6515e-05 - accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 6.9830e-05 - accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 6.9964e-05 - accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "63/63 [==============================] - 15s 235ms/step - loss: 3.2256e-05 - accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "63/63 [==============================] - 15s 234ms/step - loss: 2.3619e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=25, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 5s 73ms/step - loss: 2.7258 - accuracy: 0.5675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.725839853286743, 0.5675438642501831]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/resnet50.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('model/resnet50_training_history.npy', history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 7s 23ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.5952    0.7463        42\n",
      "           1     1.0000    0.3810    0.5517        42\n",
      "           2     0.5323    0.7857    0.6346        42\n",
      "           3     0.7174    0.7857    0.7500        42\n",
      "           4     0.6885    1.0000    0.8155        42\n",
      "           5     0.5122    1.0000    0.6774        42\n",
      "           6     0.4828    1.0000    0.6512        42\n",
      "           7     0.0769    0.0476    0.0588        42\n",
      "           8     0.7500    1.0000    0.8571        42\n",
      "           9     0.4932    0.8571    0.6261        42\n",
      "          10     0.3471    1.0000    0.5153        42\n",
      "          11     0.5789    0.5238    0.5500        42\n",
      "          12     0.0000    0.0000    0.0000        42\n",
      "          13     0.7170    0.9048    0.8000        42\n",
      "          14     0.5968    0.8810    0.7115        42\n",
      "          15     0.7778    1.0000    0.8750        42\n",
      "          16     0.6562    1.0000    0.7925        42\n",
      "          17     1.0000    1.0000    1.0000        42\n",
      "          18     0.4157    0.8810    0.5649        42\n",
      "          19     0.8571    1.0000    0.9231        42\n",
      "          20     0.5753    1.0000    0.7304        42\n",
      "          21     0.7619    0.7619    0.7619        42\n",
      "          22     0.5316    1.0000    0.6942        42\n",
      "          23     1.0000    0.0952    0.1739        42\n",
      "          24     0.7069    0.9762    0.8200        42\n",
      "          25     0.4524    0.9048    0.6032        42\n",
      "          26     0.7241    1.0000    0.8400        42\n",
      "          27     0.8750    1.0000    0.9333        42\n",
      "          28     1.0000    1.0000    1.0000        42\n",
      "          29     0.6269    1.0000    0.7706        42\n",
      "          30     0.2819    1.0000    0.4398        42\n",
      "          31     0.9302    0.9524    0.9412        42\n",
      "          32     0.8723    0.9762    0.9213        42\n",
      "          33     0.5135    0.9048    0.6552        42\n",
      "          34     0.0000    0.0000    0.0000        42\n",
      "          35     0.8500    0.4048    0.5484        42\n",
      "          36     0.3818    1.0000    0.5526        42\n",
      "          37     0.2692    1.0000    0.4242        42\n",
      "          38     0.4828    1.0000    0.6512        42\n",
      "          39     0.8182    0.4286    0.5625        42\n",
      "          40     0.9545    1.0000    0.9767        42\n",
      "          41     0.6667    0.8095    0.7312        42\n",
      "          42     0.8235    1.0000    0.9032        42\n",
      "          43     0.2456    1.0000    0.3944        42\n",
      "          44     0.2346    1.0000    0.3801        42\n",
      "          45     1.0000    0.0476    0.0909        42\n",
      "          46     0.6885    1.0000    0.8155        42\n",
      "          47     0.2095    0.5238    0.2993        42\n",
      "          48     0.6364    1.0000    0.7778        42\n",
      "          49     0.7500    1.0000    0.8571        42\n",
      "          50     0.9697    0.7619    0.8533        42\n",
      "          51     0.7174    0.7857    0.7500        42\n",
      "          52     0.4819    0.9524    0.6400        42\n",
      "          53     0.6613    0.9762    0.7885        42\n",
      "          54     0.6275    0.7619    0.6882        42\n",
      "          55     0.5250    1.0000    0.6885        42\n",
      "          56     1.0000    0.1905    0.3200        42\n",
      "          57     0.5373    0.8571    0.6606        42\n",
      "          58     0.1176    0.1905    0.1455        42\n",
      "          59     0.7143    0.1190    0.2041        42\n",
      "          60     0.5294    0.4286    0.4737        42\n",
      "          61     0.4643    0.6190    0.5306        42\n",
      "          62     0.0000    0.0000    0.0000        42\n",
      "          63     0.1538    0.0476    0.0727        42\n",
      "          64     0.8444    0.9048    0.8736        42\n",
      "          65     0.0000    0.0000    0.0000        42\n",
      "          66     0.3471    1.0000    0.5153        42\n",
      "          67     0.5556    0.1190    0.1961        42\n",
      "          68     0.8077    0.5000    0.6176        42\n",
      "          69     0.3723    0.8333    0.5147        42\n",
      "          70     0.9333    1.0000    0.9655        42\n",
      "          71     0.7500    0.5714    0.6486        42\n",
      "          72     0.5526    1.0000    0.7119        42\n",
      "          73     0.6087    1.0000    0.7568        42\n",
      "          74     0.4928    0.8095    0.6126        42\n",
      "          75     0.5195    0.9524    0.6723        42\n",
      "          76     1.0000    0.8810    0.9367        42\n",
      "          77     0.6176    1.0000    0.7636        42\n",
      "          78     0.0000    0.0000    0.0000        42\n",
      "          79     0.5195    0.9524    0.6723        42\n",
      "          80     0.8913    0.9762    0.9318        42\n",
      "          81     0.8400    1.0000    0.9130        42\n",
      "          82     0.9070    0.9286    0.9176        42\n",
      "          83     0.6667    1.0000    0.8000        42\n",
      "          84     0.7000    1.0000    0.8235        42\n",
      "          85     0.9333    1.0000    0.9655        42\n",
      "          86     0.8571    1.0000    0.9231        42\n",
      "          87     0.5185    1.0000    0.6829        42\n",
      "          88     0.7119    1.0000    0.8317        42\n",
      "          89     0.0000    0.0000    0.0000        42\n",
      "          90     0.7115    0.8810    0.7872        42\n",
      "          91     0.9130    1.0000    0.9545        42\n",
      "          92     0.8947    0.8095    0.8500        42\n",
      "          93     0.4598    0.9524    0.6202        42\n",
      "          94     0.3514    0.9286    0.5098        42\n",
      "          95     0.5942    0.9762    0.7387        42\n",
      "          96     0.3471    1.0000    0.5153        42\n",
      "          97     0.8571    1.0000    0.9231        42\n",
      "          98     0.7800    0.9286    0.8478        42\n",
      "          99     0.7179    0.6667    0.6914        42\n",
      "         100     0.0000    0.0000    0.0000        42\n",
      "         101     0.7778    1.0000    0.8750        42\n",
      "         102     1.0000    0.1667    0.2857        42\n",
      "         103     0.0000    0.0000    0.0000        42\n",
      "         104     1.0000    0.9762    0.9880        42\n",
      "         105     0.0000    0.0000    0.0000        42\n",
      "         106     0.0000    0.0000    0.0000        42\n",
      "         107     0.0000    0.0000    0.0000        42\n",
      "         108     1.0000    0.3095    0.4727        42\n",
      "         109     1.0000    0.9762    0.9880        42\n",
      "         110     0.0000    0.0000    0.0000        42\n",
      "         111     0.0000    0.0000    0.0000        42\n",
      "         112     0.4421    1.0000    0.6131        42\n",
      "         113     1.0000    0.9524    0.9756        42\n",
      "         114     0.0000    0.0000    0.0000        42\n",
      "         115     0.5500    0.2619    0.3548        42\n",
      "         116     0.0000    0.0000    0.0000        42\n",
      "         117     0.7949    0.7381    0.7654        42\n",
      "         118     0.7241    1.0000    0.8400        42\n",
      "         119     0.0000    0.0000    0.0000        42\n",
      "         120     0.0000    0.0000    0.0000        42\n",
      "         121     0.7143    0.1190    0.2041        42\n",
      "         122     0.9310    0.6429    0.7606        42\n",
      "         123     0.2500    0.0952    0.1379        42\n",
      "         124     1.0000    0.0952    0.1739        42\n",
      "         125     0.0000    0.0000    0.0000        42\n",
      "         126     0.1250    0.0476    0.0690        42\n",
      "         127     0.0000    0.0000    0.0000        42\n",
      "         128     0.0000    0.0000    0.0000        42\n",
      "         129     1.0000    0.0238    0.0465        42\n",
      "         130     1.0000    1.0000    1.0000        42\n",
      "         131     1.0000    0.0476    0.0909        42\n",
      "         132     0.5000    0.2143    0.3000        42\n",
      "         133     0.0000    0.0000    0.0000        42\n",
      "         134     1.0000    0.0238    0.0465        42\n",
      "         135     0.6667    0.1905    0.2963        42\n",
      "         136     0.9394    0.7381    0.8267        42\n",
      "         137     0.6944    0.5952    0.6410        42\n",
      "         138     0.8947    0.4048    0.5574        42\n",
      "         139     1.0000    0.3571    0.5263        42\n",
      "         140     0.0000    0.0000    0.0000        42\n",
      "         141     0.0000    0.0000    0.0000        42\n",
      "         142     0.0000    0.0000    0.0000        42\n",
      "         143     0.0000    0.0000    0.0000        42\n",
      "         144     0.0000    0.0000    0.0000        42\n",
      "         145     1.0000    0.0476    0.0909        42\n",
      "         146     1.0000    0.2619    0.4151        42\n",
      "         147     0.9545    0.5000    0.6562        42\n",
      "         148     0.0000    0.0000    0.0000        42\n",
      "         149     0.0000    0.0000    0.0000        42\n",
      "         150     1.0000    0.8095    0.8947        42\n",
      "         151     0.7143    0.7143    0.7143        42\n",
      "         152     0.6667    0.0476    0.0889        42\n",
      "         153     0.0000    0.0000    0.0000        42\n",
      "         154     0.3853    1.0000    0.5563        42\n",
      "         155     0.0000    0.0000    0.0000        42\n",
      "         156     0.4444    0.0952    0.1569        42\n",
      "         157     0.8400    1.0000    0.9130        42\n",
      "         158     0.6154    0.1905    0.2909        42\n",
      "         159     1.0000    0.0476    0.0909        42\n",
      "         160     0.8800    0.5238    0.6567        42\n",
      "         161     0.0000    0.0000    0.0000        42\n",
      "         162     1.0000    0.8095    0.8947        42\n",
      "         163     0.5882    0.2381    0.3390        42\n",
      "         164     0.0000    0.0000    0.0000        42\n",
      "         165     1.0000    0.0238    0.0465        42\n",
      "         166     0.0000    0.0000    0.0000        42\n",
      "         167     0.5000    0.0714    0.1250        42\n",
      "         168     0.0000    0.0000    0.0000        42\n",
      "         169     1.0000    0.7381    0.8493        42\n",
      "         170     0.4444    0.6667    0.5333        42\n",
      "         171     1.0000    0.1190    0.2128        42\n",
      "         172     0.0000    0.0000    0.0000        42\n",
      "         173     1.0000    0.5476    0.7077        42\n",
      "         174     0.9500    0.9048    0.9268        42\n",
      "         175     0.9130    1.0000    0.9545        42\n",
      "         176     0.5833    1.0000    0.7368        42\n",
      "         177     0.9167    0.7857    0.8462        42\n",
      "         178     1.0000    0.5952    0.7463        42\n",
      "         179     0.0000    0.0000    0.0000        42\n",
      "         180     0.0000    0.0000    0.0000        42\n",
      "         181     0.7925    1.0000    0.8842        42\n",
      "         182     0.9130    0.5000    0.6462        42\n",
      "         183     0.0000    0.0000    0.0000        42\n",
      "         184     0.9268    0.9048    0.9157        42\n",
      "         185     0.0000    0.0000    0.0000        42\n",
      "         186     0.0000    0.0000    0.0000        42\n",
      "         187     0.2781    1.0000    0.4352        42\n",
      "         188     0.2283    1.0000    0.3717        42\n",
      "         189     0.2937    1.0000    0.4541        42\n",
      "\n",
      "    accuracy                         0.5674      7980\n",
      "   macro avg     0.5525    0.5674    0.4939      7980\n",
      "weighted avg     0.5525    0.5674    0.4939      7980\n",
      "\n",
      "[[25  0  0 ...  0  0  0]\n",
      " [ 0 16  0 ...  0  0  0]\n",
      " [ 0  0 33 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 42  0  0]\n",
      " [ 0  0  0 ...  0 42  0]\n",
      " [ 0  0  0 ...  0  0 42]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdata_user5/Desktop/envs/nobo/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bigdata_user5/Desktop/envs/nobo/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/bigdata_user5/Desktop/envs/nobo/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "cm= confusion_matrix(Y_test,y_pred)\n",
    "# np.set_printoptions(precision=2)\n",
    "cr = classification_report(Y_test, y_pred, digits=4)\n",
    "print(cr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity (TPR) of the model: 0.5674185463659148\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# assume y_true is the true labels and y_pred is the predicted labels\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# calculate total true positives and false negatives\n",
    "tp = np.diag(cm)\n",
    "fn = np.sum(cm, axis=1) - tp\n",
    "\n",
    "# calculate total sensitivity (TPR)\n",
    "sensitivity = np.sum(tp) / (np.sum(tp) + np.sum(fn))\n",
    "\n",
    "print(\"Sensitivity (TPR) of the model:\", sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crossover Error Rate (CER): 0.21743512219702693\n",
      "false positive rate for the model: 0.002288790759968705\n",
      "false negative rate for the model: 0.43258145363408523\n",
      "0.43258145363408523\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# assume y_true is the true labels and y_pred is the predicted labels\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "# calculate false positives (FP), false negatives (FN), true positives (TP), and true negatives (TN)\n",
    "FP = np.sum(cm, axis=0) - np.diag(cm)\n",
    "FN = np.sum(cm, axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = np.sum(cm) - (FP + FN + TP)\n",
    "\n",
    "# calculate the False Positive Rate (FPR) and False Negative Rate (FNR)\n",
    "FPR = FP / (FP + TN)\n",
    "FNR = FN / (FN + TP)\n",
    "\n",
    "# calculate the Crossover Error Rate (CER)\n",
    "CER = (FPR + FNR) / 2\n",
    "\n",
    "print(\"Crossover Error Rate (CER):\", np.mean(CER))\n",
    "\n",
    "FPR = np.sum(FP) / np.sum(FP + TN)\n",
    "FNR = np.sum(FN) / np.sum(FN + TP)\n",
    "FDR = np.sum(FP) / np.sum(FP + TP)\n",
    "\n",
    "print(\"false positive rate for the model:\", FPR)\n",
    "print(\"false negative rate for the model:\", FNR)\n",
    "print(FDR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nobo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b2f3040836d8930dd440357ce9e4d88224c51c61d44d8fa72c07609a2b5249b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
